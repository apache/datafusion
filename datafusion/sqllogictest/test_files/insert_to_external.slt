# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at

#   http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

###################################
## INSERT to external table tests##
###################################


statement ok
CREATE EXTERNAL TABLE aggregate_test_100 (
  c1  VARCHAR NOT NULL,
  c2  TINYINT NOT NULL,
  c3  SMALLINT NOT NULL,
  c4  SMALLINT,
  c5  INT,
  c6  BIGINT NOT NULL,
  c7  SMALLINT NOT NULL,
  c8  INT NOT NULL,
  c9  BIGINT UNSIGNED NOT NULL,
  c10 VARCHAR NOT NULL,
  c11 FLOAT NOT NULL,
  c12 DOUBLE NOT NULL,
  c13 VARCHAR NOT NULL
)
STORED AS CSV
LOCATION '../../testing/data/csv/aggregate_test_100.csv'
OPTIONS ('format.has_header' 'true');


statement ok
create table dictionary_encoded_values as values
('a', arrow_cast('foo', 'Dictionary(Int32, Utf8)')), ('b', arrow_cast('bar', 'Dictionary(Int32, Utf8)'));

query TTT
describe dictionary_encoded_values;
----
column1 Utf8 YES
column2 Dictionary(Int32, Utf8) YES

statement ok
CREATE EXTERNAL TABLE dictionary_encoded_parquet_partitioned(
  a varchar,
  b varchar,
)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/parquet_types_partitioned/'
PARTITIONED BY (b);

query I
insert into dictionary_encoded_parquet_partitioned
select * from dictionary_encoded_values
----
2

query TT
select * from dictionary_encoded_parquet_partitioned order by (a);
----
a foo
b bar

statement ok
CREATE EXTERNAL TABLE dictionary_encoded_arrow_partitioned(
  a varchar,
  b varchar,
)
STORED AS arrow
LOCATION 'test_files/scratch/insert_to_external/arrow_dict_partitioned/'
PARTITIONED BY (b);

query I
insert into dictionary_encoded_arrow_partitioned
select * from dictionary_encoded_values
----
2

statement ok
CREATE EXTERNAL TABLE dictionary_encoded_arrow_test_readback(
  a varchar,
)
STORED AS arrow
LOCATION 'test_files/scratch/insert_to_external/arrow_dict_partitioned/b=bar/';

query T
select * from dictionary_encoded_arrow_test_readback;
----
b

query TT
select * from dictionary_encoded_arrow_partitioned order by (a);
----
a foo
b bar


# test_insert_into
statement ok
set datafusion.execution.target_partitions = 8;

statement ok
CREATE EXTERNAL TABLE
ordered_insert_test(a bigint, b bigint)
STORED AS csv
LOCATION 'test_files/scratch/insert_to_external/insert_to_ordered/'
WITH ORDER (a ASC, B DESC);

query TT
EXPLAIN INSERT INTO ordered_insert_test values (5, 1), (4, 2), (7,7), (7,8), (7,9), (7,10), (3, 3), (2, 4), (1, 5);
----
logical_plan
01)Dml: op=[Insert Into] table=[ordered_insert_test]
02)--Projection: column1 AS a, column2 AS b
03)----Values: (Int64(5), Int64(1)), (Int64(4), Int64(2)), (Int64(7), Int64(7)), (Int64(7), Int64(8)), (Int64(7), Int64(9))...
physical_plan
01)DataSinkExec: sink=CsvSink(file_groups=[])
02)--SortExec: expr=[a@0 ASC NULLS LAST,b@1 DESC], preserve_partitioning=[false]
03)----ProjectionExec: expr=[column1@0 as a, column2@1 as b]
04)------ValuesExec

query I
INSERT INTO ordered_insert_test values (5, 1), (4, 2), (7,7), (7,8), (7,9), (7,10), (3, 3), (2, 4), (1, 5);
----
9

query II
SELECT * from ordered_insert_test;
----
1 5
2 4
3 3
4 2
5 1
7 10
7 9
7 8
7 7

# test partitioned insert

statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test(a string, b string, c bigint)
STORED AS csv
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned/'
PARTITIONED BY (a, b);

#note that partitioned cols are moved to the end so value tuples are (c, a, b)
query I
INSERT INTO partitioned_insert_test values (1, 10, 100), (1, 10, 200), (1, 20, 100), (1, 20, 200), (2, 20, 100), (2, 20, 200);
----
6

query ITT
select * from partitioned_insert_test order by a,b,c
----
1 10 100
1 10 200
1 20 100
2 20 100
1 20 200
2 20 200

statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test_verify(c bigint)
STORED AS csv
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned/a=20/b=100/';

query I
select * from partitioned_insert_test_verify;
----
1
2

statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test_hive(c bigint)
STORED AS csv
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned'
PARTITIONED BY (a string, b string);

query I
INSERT INTO partitioned_insert_test_hive VALUES (3,30,300);
----
1

query ITT
SELECT * FROM partitioned_insert_test_hive order by a,b,c;
----
1 10 100
1 10 200
1 20 100
2 20 100
1 20 200
2 20 200
3 30 300


statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test_json(a string, b string)
STORED AS json
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned_json/'
PARTITIONED BY (a);

query I
INSERT INTO partitioned_insert_test_json values (1, 2), (3, 4), (5, 6), (1, 2), (3, 4), (5, 6);
----
6

query TT
select * from partitioned_insert_test_json order by a,b
----
1 2
1 2
3 4
3 4
5 6
5 6

statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test_verify_json(b string)
STORED AS json
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned_json/a=2/';

query T
select * from partitioned_insert_test_verify_json;
----
1
1

statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test_pq(a string, b bigint)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned_pq/'
PARTITIONED BY (a);

query I
INSERT INTO partitioned_insert_test_pq values (1, 2), (3, 4), (5, 6), (1, 2), (3, 4), (5, 6);
----
6

query IT
select * from partitioned_insert_test_pq order by a ASC, b ASC
----
1 2
1 2
3 4
3 4
5 6
5 6

statement ok
CREATE EXTERNAL TABLE
partitioned_insert_test_verify_pq(b bigint)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/insert_to_partitioned_pq/a=2/';

query I
select * from partitioned_insert_test_verify_pq;
----
1
1


statement ok
CREATE EXTERNAL TABLE
single_file_test(a bigint, b bigint)
STORED AS csv
LOCATION 'test_files/scratch/insert_to_external/single_csv_table.csv';

query error DataFusion error: Error during planning: Inserting into a ListingTable backed by a single file is not supported, URL is possibly missing a trailing `/`\. To append to an existing file use StreamTable, e\.g\. by using CREATE UNBOUNDED EXTERNAL TABLE
INSERT INTO single_file_test values (1, 2), (3, 4);

statement ok
drop table single_file_test;

statement ok
CREATE UNBOUNDED EXTERNAL TABLE
single_file_test(a bigint, b bigint)
STORED AS csv
LOCATION 'test_files/scratch/insert_to_external/single_csv_table.csv';

query I
INSERT INTO single_file_test values (1, 2), (3, 4);
----
2

query I
INSERT INTO single_file_test values (4, 5), (6, 7);
----
2

query II
select * from single_file_test;
----
1 2
3 4
4 5
6 7

statement ok
CREATE EXTERNAL TABLE
directory_test(a bigint, b bigint)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q0/';

query I
INSERT INTO directory_test values (1, 2), (3, 4);
----
2

query II
select * from directory_test;
----
1 2
3 4

statement ok
CREATE EXTERNAL TABLE
table_without_values(field1 BIGINT NULL, field2 BIGINT NULL)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q1/';

query TT
EXPLAIN
INSERT INTO table_without_values SELECT
SUM(c4) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING),
COUNT(*) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)
FROM aggregate_test_100
ORDER by c1
----
logical_plan
01)Dml: op=[Insert Into] table=[table_without_values]
02)--Projection: sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING AS field1, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING AS field2
03)----Sort: aggregate_test_100.c1 ASC NULLS LAST
04)------Projection: sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING, aggregate_test_100.c1
05)--------WindowAggr: windowExpr=[[sum(CAST(aggregate_test_100.c4 AS Int64)) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING, count(Int64(1)) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING AS count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING]]
06)----------TableScan: aggregate_test_100 projection=[c1, c4, c9]
physical_plan
01)DataSinkExec: sink=ParquetSink(file_groups=[])
02)--ProjectionExec: expr=[sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING@0 as field1, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING@1 as field2]
03)----SortPreservingMergeExec: [c1@2 ASC NULLS LAST]
04)------ProjectionExec: expr=[sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING@3 as sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING@4 as count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING, c1@0 as c1]
05)--------BoundedWindowAggExec: wdw=[sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING: Ok(Field { name: "sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING", data_type: Int64, nullable: true, dict_id: 0, dict_is_ordered: false, metadata: {} }), frame: WindowFrame { units: Rows, start_bound: Preceding(UInt64(1)), end_bound: Following(UInt64(1)), is_causal: false }, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING: Ok(Field { name: "count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING", data_type: Int64, nullable: false, dict_id: 0, dict_is_ordered: false, metadata: {} }), frame: WindowFrame { units: Rows, start_bound: Preceding(UInt64(1)), end_bound: Following(UInt64(1)), is_causal: false }], mode=[Sorted]
06)----------SortExec: expr=[c1@0 ASC NULLS LAST,c9@2 ASC NULLS LAST], preserve_partitioning=[true]
07)------------CoalesceBatchesExec: target_batch_size=8192
08)--------------RepartitionExec: partitioning=Hash([c1@0], 8), input_partitions=8
09)----------------RepartitionExec: partitioning=RoundRobinBatch(8), input_partitions=1
10)------------------CsvExec: file_groups={1 group: [[WORKSPACE_ROOT/testing/data/csv/aggregate_test_100.csv]]}, projection=[c1, c4, c9], has_header=true

query I
INSERT INTO table_without_values SELECT
SUM(c4) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING),
COUNT(*) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)
FROM aggregate_test_100
ORDER by c1
----
100

# verify there is data now in the table
query I
SELECT COUNT(*) from table_without_values;
----
100

# verify there is data now in the table
query II
SELECT *
FROM table_without_values
ORDER BY field1, field2
LIMIT 5;
----
-70111 3
-65362 3
-62295 3
-56721 3
-55414 3

statement ok
drop table table_without_values;

# test_insert_into_as_select_multi_partitioned
statement ok
CREATE EXTERNAL TABLE
table_without_values(field1 BIGINT NULL, field2 BIGINT NULL)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q2/';

query TT
EXPLAIN
INSERT INTO table_without_values SELECT
SUM(c4) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as a1,
COUNT(*) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as a2
FROM aggregate_test_100
----
logical_plan
01)Dml: op=[Insert Into] table=[table_without_values]
02)--Projection: sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING AS field1, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING AS field2
03)----WindowAggr: windowExpr=[[sum(CAST(aggregate_test_100.c4 AS Int64)) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING, count(Int64(1)) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING AS count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING]]
04)------TableScan: aggregate_test_100 projection=[c1, c4, c9]
physical_plan
01)DataSinkExec: sink=ParquetSink(file_groups=[])
02)--CoalescePartitionsExec
03)----ProjectionExec: expr=[sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING@3 as field1, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING@4 as field2]
04)------BoundedWindowAggExec: wdw=[sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING: Ok(Field { name: "sum(aggregate_test_100.c4) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING", data_type: Int64, nullable: true, dict_id: 0, dict_is_ordered: false, metadata: {} }), frame: WindowFrame { units: Rows, start_bound: Preceding(UInt64(1)), end_bound: Following(UInt64(1)), is_causal: false }, count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING: Ok(Field { name: "count(*) PARTITION BY [aggregate_test_100.c1] ORDER BY [aggregate_test_100.c9 ASC NULLS LAST] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING", data_type: Int64, nullable: false, dict_id: 0, dict_is_ordered: false, metadata: {} }), frame: WindowFrame { units: Rows, start_bound: Preceding(UInt64(1)), end_bound: Following(UInt64(1)), is_causal: false }], mode=[Sorted]
05)--------SortExec: expr=[c1@0 ASC NULLS LAST,c9@2 ASC NULLS LAST], preserve_partitioning=[true]
06)----------CoalesceBatchesExec: target_batch_size=8192
07)------------RepartitionExec: partitioning=Hash([c1@0], 8), input_partitions=8
08)--------------RepartitionExec: partitioning=RoundRobinBatch(8), input_partitions=1
09)----------------CsvExec: file_groups={1 group: [[WORKSPACE_ROOT/testing/data/csv/aggregate_test_100.csv]]}, projection=[c1, c4, c9], has_header=true



query I
INSERT INTO table_without_values SELECT
SUM(c4) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as a1,
COUNT(*) OVER(PARTITION BY c1 ORDER BY c9 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as a2
FROM aggregate_test_100
----
100

statement ok
drop table table_without_values;


# test_insert_into_with_sort
statement ok
CREATE EXTERNAL TABLE
table_without_values(c1 varchar NULL)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q3/';

# verify that the sort order of the insert query is maintained into the
# insert (there should be a SortExec in the following plan)
# See https://github.com/apache/datafusion/pull/6354#discussion_r1195284178 for more background
query TT
explain insert into table_without_values select c1 from aggregate_test_100 order by c1;
----
logical_plan
01)Dml: op=[Insert Into] table=[table_without_values]
02)--Projection: aggregate_test_100.c1 AS c1
03)----Sort: aggregate_test_100.c1 ASC NULLS LAST
04)------TableScan: aggregate_test_100 projection=[c1]
physical_plan
01)DataSinkExec: sink=ParquetSink(file_groups=[])
02)--SortExec: expr=[c1@0 ASC NULLS LAST], preserve_partitioning=[false]
03)----CsvExec: file_groups={1 group: [[WORKSPACE_ROOT/testing/data/csv/aggregate_test_100.csv]]}, projection=[c1], has_header=true

query I
insert into table_without_values select c1 from aggregate_test_100 order by c1;
----
100

query I
select count(*) from table_without_values;
----
100


statement ok
drop table table_without_values;


# test insert with column names
statement ok
CREATE EXTERNAL TABLE
table_without_values(id BIGINT, name varchar)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q4/';

query I
insert into table_without_values(id, name) values(1, 'foo');
----
1

query I
insert into table_without_values(name, id) values('bar', 2);
----
1

statement error Schema error: Schema contains duplicate unqualified field name id
insert into table_without_values(id, id) values(3, 3);

statement error Arrow error: Cast error: Cannot cast string 'zoo' to value of Int64 type
insert into table_without_values(name, id) values(4, 'zoo');

statement error Error during planning: Column count doesn't match insert query!
insert into table_without_values(id) values(4, 'zoo');

# insert NULL values for the missing column (name)
query I
insert into table_without_values(id) values(4);
----
1

query IT rowsort
select * from table_without_values;
----
1 foo
2 bar
4 NULL

statement ok
drop table table_without_values;

# test insert with non-nullable column
statement ok
CREATE EXTERNAL TABLE
table_without_values(field1 BIGINT NOT NULL, field2 BIGINT NULL)
STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q5/';

query I
insert into table_without_values values(1, 100);
----
1

query I
insert into table_without_values values(2, NULL);
----
1

# insert NULL values for the missing column (field2)
query I
insert into table_without_values(field1) values(3);
----
1

# insert NULL values for the missing column (field1), but column is non-nullable
statement error Execution error: Invalid batch column at '0' has null but schema specifies non-nullable
insert into table_without_values(field2) values(300);

statement error Execution error: Invalid batch column at '0' has null but schema specifies non-nullable
insert into table_without_values values(NULL, 300);

statement error Execution error: Invalid batch column at '0' has null but schema specifies non-nullable
insert into table_without_values values(3, 300), (NULL, 400);

query II rowsort
select * from table_without_values;
----
1 100
2 NULL
3 NULL

statement ok
drop table table_without_values;


### Test for specifying column's default value

statement ok
CREATE EXTERNAL TABLE test_column_defaults(
  a int,
  b int not null default null,
  c int default 100*2+300,
  d text default lower('DEFAULT_TEXT'),
  e timestamp default now()
) STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q6/';

# fill in all column values
query I
insert into test_column_defaults values(1, 10, 100, 'ABC', now())
----
1

statement error DataFusion error: Execution error: Invalid batch column at '1' has null but schema specifies non-nullable
insert into test_column_defaults(a) values(2)

query I
insert into test_column_defaults(b) values(20)
----
1

query IIIT rowsort
select a,b,c,d from test_column_defaults
----
1 10 100 ABC
NULL 20 500 default_text

# fill the timestamp column with default value `now()` again, it should be different from the previous one
query I
insert into test_column_defaults(a, b, c, d) values(2, 20, 200, 'DEF')
----
1

# Ensure that the default expression `now()` is evaluated during insertion, not optimized away.
# Rows are inserted during different time, so their timestamp values should be different.
query I rowsort
select count(distinct e) from test_column_defaults
----
3

# Expect all rows to be true as now() was inserted into the table
query B rowsort
select e < now() from test_column_defaults
----
true
true
true

statement ok
drop table test_column_defaults

# test invalid default value
statement error DataFusion error: Error during planning: Column reference is not allowed in the DEFAULT expression : Schema error: No field named a.
CREATE EXTERNAL TABLE test_column_defaults(
  a int,
  b int default a+1
) STORED AS parquet
LOCATION 'test_files/scratch/insert_to_external/external_parquet_table_q7/';
