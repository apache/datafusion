# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.
#
# The ASF licenses this file to you under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# -----------------------------------------------------------------------------
# Example metadata used to generate documentation and validate examples in CI.
#
# - Each section defines an example group (invoked via `cargo run --example <group>`).
# - Keys are subcommand names passed after `--` (they do NOT need to match filenames).
# - `file` must refer to an existing `examples/<group>/*.rs` source file.
# - Files present on disk but missing here will appear in generated docs with TODOs.
# -----------------------------------------------------------------------------

[builtin_functions]
date_time = { file = "date_time.rs", desc = "Examples of date-time related functions and queries" }
function_factory = { file = "function_factory.rs", desc = "Register `CREATE FUNCTION` handler to implement SQL macros" }
regexp = { file = "regexp.rs", desc = "Examples of using regular expression functions" }

[custom_data_source]
csv_sql_streaming = { file = "csv_sql_streaming.rs", desc = "Run a streaming SQL query against CSV data" }
csv_json_opener = { file = "csv_json_opener.rs", desc = "Use low-level FileOpener APIs for CSV/JSON" }
custom_datasource = { file = "custom_datasource.rs", desc = "Query a custom TableProvider" }
custom_file_casts = { file = "custom_file_casts.rs", desc = "Implement custom casting rules" }
custom_file_format = { file = "custom_file_format.rs", desc = "Write to a custom file format" }
default_column_values = { file = "default_column_values.rs", desc = "Custom default values using metadata" }
file_stream_provider = { file = "file_stream_provider.rs", desc = "Read/write via FileStreamProvider for streams" }

[data_io]
catalog = { file = "catalog.rs", desc = "Register tables into a custom catalog" }
json_shredding = { file = "json_shredding.rs", desc = "Implement filter rewriting for JSON shredding" }
parquet_adv_idx = { file = "parquet_advanced_index.rs", desc = "Create a secondary index across multiple parquet files" }
parquet_emb_idx = { file = "parquet_embedded_index.rs", desc = "Store a custom index inside Parquet files" }
parquet_enc = { file = "parquet_encrypted.rs", desc = "Read & write encrypted Parquet files" }
parquet_enc_with_kms = { file = "parquet_encrypted_with_kms.rs", desc = "Encrypted Parquet I/O using a KMS-backed factory" }
parquet_exec_visitor = { file = "parquet_exec_visitor.rs", desc = "Extract statistics by visiting an ExecutionPlan" }
parquet_idx = { file = "parquet_index.rs", desc = "Create a secondary index" }
query_http_csv = { file = "query_http_csv.rs", desc = "Query CSV files via HTTP" }
remote_catalog = { file = "remote_catalog.rs", desc = "Interact with a remote catalog" }

[dataframe]
cache_factory = { file = "cache_factory.rs", desc = "Custom lazy caching for DataFrames using `CacheFactory`" }
dataframe = { file = "dataframe.rs", desc = "Query DataFrames from various sources and write output" }
deserialize_to_struct = { file = "deserialize_to_struct.rs", desc = "Convert Arrow arrays into Rust structs" }

[execution_monitoring]
mem_pool_exec_plan = { file = "memory_pool_execution_plan.rs", desc = "Memory-aware ExecutionPlan with spilling" }
mem_pool_tracking = { file = "memory_pool_tracking.rs", desc = "Demonstrates memory tracking" }
tracing = { file = "tracing.rs", desc = "Demonstrates tracing integration" }

[external_dependency]
dataframe_to_s3 = { file = "dataframe_to_s3.rs", desc = "Query DataFrames and write results to S3" }
query_aws_s3 = { file = "query_aws_s3.rs", desc = "Query S3-backed data using object_store" }

[flight]
server = { file = "server.rs", desc = "Run DataFusion server accepting FlightSQL/JDBC queries" }
client = { file = "client.rs", desc = "Execute SQL queries via Arrow Flight protocol" }
sql_server = { file = "sql_server.rs", desc = "Standalone SQL server for JDBC clients" }

[proto]
composed_extension_codec = { file = "composed_extension_codec.rs", desc = "Use multiple extension codecs for serialization/deserialization" }

[query_planning]
analyzer_rule = { file = "analyzer_rule.rs", desc = "Custom AnalyzerRule to change query semantics" }
expr_api = { file = "expr_api.rs", desc = "Create, execute, analyze, and coerce Exprs" }
optimizer_rule = { file = "optimizer_rule.rs", desc = "Replace predicates via a custom OptimizerRule" }
parse_sql_expr = { file = "parse_sql_expr.rs", desc = "Parse SQL into DataFusion Expr" }
plan_to_sql = { file = "plan_to_sql.rs", desc = "Generate SQL from expressions or plans" }
planner_api = { file = "planner_api.rs", desc = "APIs for logical and physical plan manipulation" }
pruning = { file = "pruning.rs", desc = "Use pruning to skip irrelevant files" }
thread_pools = { file = "thread_pools.rs", desc = "Configure custom thread pools for DataFusion execution" }

[relation_planner]
match_recognize = { file = "match_recognize.rs", desc = "Implement MATCH_RECOGNIZE pattern matching" }
pivot_unpivot = { file = "pivot_unpivot.rs", desc = "Implement PIVOT / UNPIVOT" }
table_sample = { file = "table_sample.rs", desc = "Implement TABLESAMPLE" }

[sql_ops]
analysis = { file = "analysis.rs", desc = "Analyze SQL queries" }
custom_sql_parser = { file = "custom_sql_parser.rs", desc = "Implement a custom SQL parser to extend DataFusion" }
frontend = { file = "frontend.rs", desc = "Build LogicalPlans from SQL" }
query = { file = "query.rs", desc = "Query data using SQL" }

[udf]
adv_udaf = { file = "advanced_udaf.rs", desc = "Advanced User Defined Aggregate Function (UDAF)" }
adv_udf = { file = "advanced_udf.rs", desc = "Advanced User Defined Scalar Function (UDF)" }
adv_udwf = { file = "advanced_udwf.rs", desc = "Advanced User Defined Window Function (UDWF)" }
async_udf = { file = "async_udf.rs", desc = "Asynchronous User Defined Scalar Function" }
udaf = { file = "simple_udaf.rs", desc = "Simple UDAF example" }
udf = { file = "simple_udf.rs", desc = "Simple UDF example" }
udtf = { file = "simple_udtf.rs", desc = "Simple UDTF example" }
udwf = { file = "simple_udwf.rs", desc = "Simple UDWF example" }
